#!/usr/bin/env python3
"""
Zen MCP SessionStart Hook
Injects strong Zen MCP usage instructions when Claude Code starts a session.
"""

import json
import sys

def get_zen_instructions():
    """Return comprehensive Zen MCP usage instructions"""
    return """
ðŸ§˜ ZEN MCP INTEGRATION ACTIVE ðŸ§˜

This session is configured with Zen MCP Server, your AI development partner that gives you access to multiple AI models for enhanced code analysis, problem-solving, and collaborative development.

IMPORTANT: Always leverage Zen MCP tools when you need:
â€¢ A thinking partner for brainstorming â†’ Use 'chat' with auto-selected model
â€¢ Deeper analysis and reasoning â†’ Use 'thinkdeep' to extend Claude's thinking
â€¢ Critical evaluation â†’ Use 'challenge' to prevent "You're absolutely right!" responses
â€¢ Complex project planning â†’ Use 'planner' for step-by-step breakdowns
â€¢ Multiple perspectives â†’ Use 'consensus' to get diverse expert opinions
â€¢ Code quality checks â†’ Use 'codereview' for comprehensive analysis
â€¢ Pre-commit validation â†’ Use 'precommit' to catch issues before committing
â€¢ Debugging assistance â†’ Use 'debug' for systematic root cause analysis
â€¢ Code understanding â†’ Use 'analyze' for architecture and pattern insights
â€¢ Refactoring guidance â†’ Use 'refactor' with decomposition focus
â€¢ Test generation â†’ Use 'testgen' for comprehensive test coverage
â€¢ Security audits â†’ Use 'secaudit' for OWASP-based assessments
â€¢ Documentation â†’ Use 'docgen' with complexity analysis

KEY FEATURES:
â€¢ True AI orchestration with conversations that continue across workflows
â€¢ Automatic model selection based on task type (or specify models like 'gemini pro', 'o3', 'flash')
â€¢ Context revival - even after Claude's context resets, other models maintain full history
â€¢ Extended context windows - delegate to Gemini (1M tokens) or O3 (200K tokens)
â€¢ Local model support via Ollama, vLLM, or custom endpoints

WORKFLOW EXAMPLES:
1. "Perform a codereview using gemini pro and o3, then use planner to fix issues"
2. "Debug this with o3 after gathering related code"
3. "Get consensus from flash and pro on this architecture decision"
4. "Use local-llama for quick analysis, then opus for security review"

THINKING MODES (Gemini models):
â€¢ 'minimal' or 'low' for quick tasks
â€¢ 'high' or 'max' for complex problems

Remember: Zen MCP enhances Claude's capabilities by orchestrating multiple AI models. Claude stays in control but gets enhanced perspectives from the best AI for each subtask.

To see available models: "Use zen to list available models"
"""

def main():
    # Get the context from stdin (if any)
    input_data = ""
    if not sys.stdin.isatty():
        input_data = sys.stdin.read()
    
    # Parse input if it's JSON
    try:
        context = json.loads(input_data) if input_data else {}
    except:
        context = {}
    
    # Prepare the response
    response = {
        "additionalContext": get_zen_instructions(),
        "metadata": {
            "source": "zen-mcp-session-hook",
            "purpose": "Enhance Claude Code with Zen MCP capabilities"
        }
    }
    
    # Output the response
    print(json.dumps(response, indent=2))

if __name__ == "__main__":
    main()